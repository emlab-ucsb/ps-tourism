---
output:
  html_document: default

title: 'diveboard.com data: Processing'
author: "Kat Millage"
date: "`r format(Sys.time(), '%B %d, %Y')`"
fontsize: 11pt
---

```{r setup, include=FALSE}
# Packages
#library(readr)
library(DBI)
library(bigrquery)

# Source common R file
source(here::here("common.R"))
```

# Introduction

This script details the processing of the crowdsourced database obtained from diveboard.com. 

## First Attempt

All of the raw data we obtained from diveboard.com is stored on in emLab Google Drive "data" folder. All of the data are stored in .sql files, which is going to require a little bit of wrangling. Quick search yields the following... 

```{r}
# # Original file is stored on Google Shared Drive (DO NOT EDIT DIRECTLY)
# diveboard_dat_path <- "/Volumes/GoogleDrive/Shared\ drives/emlab/data/diveboard-dive-sites/"
# 
# # There's no metadata, so let's just start with the first file and see what's there...
# countries_dat_test <- read_file(paste0(diveboard_dat_path, "countries.sql"))
# 
# # So we get a wonderful jumble of t ext, but clearly this is a table of country names and coordinates... Let's try the other function just to see if we get something prettier. (Solution from https://newbedev.com/how-to-read-the-contents-of-an-sql-file-into-an-r-script-to-run-a-query)
# getSQL <- function(filepath){
#   con = file(filepath, "r")
#   sql.string <- ""
# 
#   while (TRUE){
#     line <- readLines(con, n = 1)
# 
#     if ( length(line) == 0 ){
#       break
#     }
# 
#     line <- gsub("\\t", " ", line)
# 
#     if(grepl("--",line) == TRUE){
#       line <- paste(sub("--","/*",line),"*/")
#     }
# 
#     sql.string <- paste(sql.string, line)
#   }
# 
#   close(con)
#   return(sql.string)
# }
# 
# countries_dat_test2 <- getSQL(paste0(diveboard_dat_path, "countries.sql"))
```

Neither of those approaches to directly read in the data worked. The raw data appear to be SQL "dumps" generated by HeidiSQL. It looks like the best course of action is going to be to upload them into some sort of SQL database that can read them, and then transfer them to BigQuery so we can connect to them from here in a way we're familiar with. 

# Upload Process

We first uploaded the following data files (provided by Alexander Casassovici at diveboard.com) to a Google Cloud storage bucket owned by the `emlab-gcp` account:
- countries.sql
- dives_eolcnames.sql
- dives_filtered.sql*
- dives_fish.sql
- eolcnames.sql
- eolsnames.sql
- locations_regions.sql
- locations.sql
- regions.sql
- shops.sql
- spots.sql
- users_filtered.sql*

We then created a Google Cloud SQL (MySQL) instance called "diveboard" in which we then created a new database called "diveboard". We then manually imported the data files above into this database. There were two files (dives_filtered.sql and users_filtered.sql) that were unable to be imported. Both files appear to be missing all of the header information specifying schema, etc. We will deal with these two files separately at the end of this script. 

From there, we were able to copy the tables that were imported successfully into bigquery using guidance from the following sources: 
- [Best way to import Google cloud SQL data into Bigquery](https://hoffa.medium.com/loading-mysql-backup-files-into-bigquery-straight-from-cloud-sql-d40a98281229)
- [Loading MySQL backup files into BigQuery - straight out of Cloud SQL](https://stackoverflow.com/questions/47624181/best-way-to-import-google-cloud-sql-data-into-bigquery?rq=1)

Now we should be able to access them and manipulate those data from within using the `rbigquery` package. 

Note: It might be possible to connect directly to the Cloud SQL database (see [Databases using R](https://db.rstudio.com/getting-started/database-queries/)). 

```{r}
## Setup our credentials to access BigQuery
# Bigquery project
bq_project <-  "emlab-gcp"

# Solution to bigrquery error (August 5, 2020) - see https://github.com/r-dbi/bigrquery/issues/395 for details
options(scipen = 20)

# Authenticate rbigquery
bq_auth(path = "/Users/kat/Github/emlab-gcp-ad2095cf4881.json")
```

## countries

The raw `countries.sql` file contains 254 rows and 8 columns: 
  - id
  - cname
  - ccode
  - created_at
  - updated_at
  - blob
  - nesw_bounds
  - best_pic_ids
  
For ease I've previewed the contents of these files using the BigQuery web browser, and now I'm going to extract only the columns I think we need and save those into the emLab GCP. This table is also rather small, and clearly is a country lookup table so let's go ahead and download it into local memory too. 

```{r}
countries_sql <- "
  SELECT
      id,
      cname,
      ccode,
      blob,
      nesw_bounds
    FROM
     EXTERNAL_QUERY('projects/emlab-gcp/locations/us-west2/connections/diveboard',
    'SELECT * FROM diveboard.countries;')"

countries_table <- bq_table(project = bq_project, 
                            table = "countries",
                            dataset = "diveboard")

if(bq_table_exists(countries_table)){
  
  bq_table_delete(countries_table)
  countries_df <- 
    bq_project_query(bq_project, 
                     countries_sql,
                     destination_table = countries_table,
                     use_legacy_sql = FALSE,
                     allowLargeResults = TRUE) %>%
    bq_table_download()
    
  
}else {
  bq_table_create(countries_table)
  countries_df <- 
    bq_project_query(bq_project, 
                     query = countries_sql,
                     destination_table = countries_table,
                     use_legacy_sql = FALSE,
                     allowLargeResults = TRUE) %>%
    bq_table_download()
}

# Save to emLab GD
write_csv(countries_df, file.path(emlab_project_dir, "data", "01-raw", "diveboard", "countries.csv"))
```

## dives_eolcnames

The raw `dives_eolcnames.sql` file contains 45,587 rows and 3 columns: 
- dive_id
- sname_id
- cname_id: guessing this is the country id

Not sure yet what all of this means, so we'll save all of it to the emlab GCP.

```{r}
dives_eolcnames_sql <- "
  SELECT
      *
    FROM
     EXTERNAL_QUERY('projects/emlab-gcp/locations/us-west2/connections/diveboard',
    'SELECT * FROM diveboard.dives_eolcnames;')"

dives_eolcnames_table <- bq_table(project = bq_project, 
                                  table = "dives_eolcnames",
                                  dataset = "diveboard")

if(bq_table_exists(dives_eolcnames_table)){
  
  bq_table_delete(dives_eolcnames_table)
  dives_eolcnames_df <- 
    bq_project_query(bq_project, 
                   dives_eolcnames_sql,
                   destination_table = dives_eolcnames_table,
                   use_legacy_sql = FALSE,
                   allowLargeResults = TRUE) %>%
    bq_table_download()

}else {
  bq_table_create(dives_eolcnames_table)
  dives_eolcnames_df <- 
    bq_project_query(bq_project, 
                   query = dives_eolcnames_sql, 
                   destination_table = dives_eolcnames_table,
                   use_legacy_sql = FALSE, 
                   allowLargeResults = TRUE) %>%
    bq_table_download()
}

# Save to emLab GD
write_csv(dives_eolcnames_df, file.path(emlab_project_dir, "data", "01-raw", "diveboard", "dives_eolcnames.csv"))
```

## dives_fish

The raw `dives_fish.sql` file contains 30 rows and 2 columns: 
- dive_id
- fish_id

This is clearly a lookup table and very small, so let's save the entire contents into the emLab GCP and download it into local memory. 

```{r}
dives_fish_sql <- "
  SELECT
      *
    FROM
     EXTERNAL_QUERY('projects/emlab-gcp/locations/us-west2/connections/diveboard',
    'SELECT * FROM diveboard.dives_fish;')"

dives_fish_table <- bq_table(project = bq_project, 
                                  table = "dives_fish",
                                  dataset = "diveboard")

if(bq_table_exists(dives_fish_table)){
  
  bq_table_delete(dives_fish_table)
  dives_fish_df <- 
    bq_project_query(bq_project, 
                     dives_fish_sql,
                     destination_table = dives_fish_table,
                     use_legacy_sql = FALSE,
                     allowLargeResults = TRUE) %>%
    bq_table_download()
    
  
}else {
  bq_table_create(dives_fish_table)
  dives_fish_df <- 
    bq_project_query(bq_project, 
                     query = dives_fish_sql,
                     destination_table = dives_fish_table,
                     use_legacy_sql = FALSE,
                     allowLargeResults = TRUE) %>%
    bq_table_download()
}

# Save to emlab GD
write_csv(dives_fish_df, file.path(emlab_project_dir, "data", "01-raw", "diveboard", "dives_fish.csv"))
```

## eolcnames

The raw `eolcnames.sql` file contains 349,980 rows and 7 columns: 
- id
- eolsname_id
- cname
- language
- eol_preferred: binary, 1 is yes, 0 no
- created_at
- upaded_at

Not sure yet what all of this means, so we'll save all of it to the emlab GCP.

```{r}
eolcnames_sql <- "
  SELECT
      *
    FROM
     EXTERNAL_QUERY('projects/emlab-gcp/locations/us-west2/connections/diveboard',
    'SELECT * FROM diveboard.eolcnames;')"

eolcnames_table <- bq_table(project = bq_project, 
                            table = "eolcnames",
                            dataset = "diveboard")

if(bq_table_exists(eolcnames_table)){
  
  bq_table_delete(eolcnames_table)
  eolcnames_df <- 
    bq_project_query(bq_project, 
                   eolcnames_sql,
                   destination_table = eolcnames_table,
                   use_legacy_sql = FALSE,
                   allowLargeResults = TRUE) %>%
    bq_table_download()

}else {
  bq_table_create(eolcnames_table)
  eolcnames_df <- 
    bq_project_query(bq_project, 
                   query = eolcnames_sql, 
                   destination_table = eolcnames_table,
                   use_legacy_sql = FALSE, 
                   allowLargeResults = TRUE) %>%
    bq_table_download()
}

# Save to emlab GD
write_csv(eolcnames_df, file.path(emlab_project_dir, "data", "01-raw", "diveboard", "eolcnames.csv"))
```

## eolsnames

The raw `eolsnames.sql` file contains 123,714 rows and 23 columns: 
- id
- sname
- taxon: Contains additional nested information on scientific name and identifier according to WORM Species Information, FishBase, IUCN Red List, NCBI Taxonomy, Species 2000 & ITIS Catalogue of Life: Annual Checklis 2010, and Integrated Taxonomic Information System (ITIS). There may be more - it's difficult to tell if all entries contain the same subset of sources. 
- data
- picture
- created_at
- updated_at
- gbif_id
- worms_id
- fishbase_id
- worms_parent_id
- fishbase_parent_id
- fishbase_taxonrank
- worms_heirarchy
- fishbase_hierarchy
- category
- taxonrank
- parent_id
- has_occurences
- eol_description
- thumbnail_href
- category_inspire

This is clearly the species lookup table. I don't think we need to keep all of these for our purposes... Until I decide what we really need, let's just keep this on the emLab GCP. 

```{r}
eolsnames_sql <- "
  SELECT
      *
    FROM
     EXTERNAL_QUERY('projects/emlab-gcp/locations/us-west2/connections/diveboard',
    'SELECT * FROM diveboard.eolsnames;')"

eolsnames_table <- bq_table(project = bq_project, 
                                  table = "eolsnames",
                                  dataset = "diveboard")

if(bq_table_exists(eolsnames_table)){
  
  bq_table_delete(eolsnames_table)
  eolsnames_df <- 
    bq_project_query(bq_project, 
                     eolsnames_sql,
                     eolsnames_sql = eolsnames_table,
                     use_legacy_sql = FALSE,
                     allowLargeResults = TRUE) %>%
    bq_table_download()
    
  
}else {
  bq_table_create(eolsnames_table)
  eolsnames_df <- 
    bq_project_query(bq_project, 
                     query = eolsnames_sql,
                     destination_table = eolsnames_table,
                     use_legacy_sql = FALSE,
                     allowLargeResults = TRUE) %>%
    bq_table_download()
}

# Save to emlab GD
write_csv(eolsnames_df, file.path(emlab_project_dir, "data", "01-raw", "diveboard", "eolsnames.csv"))
```

## locations_regions

The raw `locations_regions.sql` file contains 52,294,223 rows and 2 columns: 
- location_id
- region_id

This is clearly a lookup table, but there's a ton of repetition... If we use `SELECT DISTINCT`, we get 117,512 rows instead. Let's save that aggregated version to the emLab GCP. 

```{r}
locations_regions_sql <- "
  SELECT DISTINCT
      location_id,
      region_id
    FROM
     EXTERNAL_QUERY('projects/emlab-gcp/locations/us-west2/connections/diveboard',
    'SELECT * FROM diveboard.locations_regions;')"

locations_regions_table <- bq_table(project = bq_project, 
                                  table = "locations_regions_distinct",
                                  dataset = "diveboard")

if(bq_table_exists(locations_regions_table)){
  
  bq_table_delete(locations_regions_table)
  locations_regions_df <- 
    bq_project_query(bq_project, 
                     locations_regions_sql,
                     eolsnames_sql = locations_regions_table,
                     use_legacy_sql = FALSE,
                     allowLargeResults = TRUE) %>%
    bq_table_download()
    
  
}else {
  bq_table_create(locations_regions_table)
  locations_regions_df <- 
    bq_project_query(bq_project, 
                     query = locations_regions_sql,
                     destination_table = locations_regions_table,
                     use_legacy_sql = FALSE,
                     allowLargeResults = TRUE) %>%
    bq_table_download()
}

# Save to emlab GD
write_csv(locations_regions_df, file.path(emlab_project_dir, "data", "01-raw", "diveboard", "locations_regions.csv"))
```

## locations

The raw `locations.sql` file contains 109,994 rows and 11 columns: 
- id
- name
- country_id
- created_at
- udpated_at
- nesw_bounds
- verified_user_id
- verified_date
- redirect_id
- best_pic_ids
- delta

This gives clear names for our dive locations - this is going to be important. I think this might be ok to download in its entirety. Let's give it a try. I don't think we need redirect_id or best_pic_ids so i'm going to eliminate those columns. 

```{r}
locations_sql <- "
  SELECT
      *
    FROM
     EXTERNAL_QUERY('projects/emlab-gcp/locations/us-west2/connections/diveboard',
    'SELECT * FROM diveboard.locations;')"

locations_table <- bq_table(project = bq_project, 
                            table = "locations",
                            dataset = "diveboard")

if(bq_table_exists(locations_table)){
  
  bq_table_delete(locations_table)
  locations_df <- 
    bq_project_query(bq_project, 
                     locations_sql,
                     destination_table = locations_table,
                     use_legacy_sql = FALSE,
                     allowLargeResults = TRUE) %>%
    bq_table_download()
    
  
}else {
  bq_table_create(locations_table)
  locations_df <- 
    bq_project_query(bq_project, 
                     query = locations_sql,
                     destination_table = locations_table,
                     use_legacy_sql = FALSE,
                     allowLargeResults = TRUE) %>%
    bq_table_download()
}

locations_df_out <- locations_df %>%
  dplyr::select(-created_at, -updated_at, -redirect_id, -best_pic_ids, -nesw_bounds, -verified_user_id, -verified_date)

# Save locally
write_csv(locations_df_out, file.path(emlab_project_dir, "data", "01-raw", "diveboard", "locations.csv"))
```

## regions

The raw `regions.sql` file contains 49,74 rows and 2 columns: 
- id
- name
- created_at
- udpated_at
- nesw_bounds
- verified_user_id
- verified_date
- redirect_id
- best_pic_ids
- delta
- geonames_core_id

This gives clear names for our dive regions - this is going to be important. Let's download this one. 

```{r}
regions_sql <- "
  SELECT
      *
    FROM
     EXTERNAL_QUERY('projects/emlab-gcp/locations/us-west2/connections/diveboard',
    'SELECT * FROM diveboard.regions;')"

regions_table <- bq_table(project = bq_project, 
                            table = "regions",
                            dataset = "diveboard")

if(bq_table_exists(regions_table)){
  
  bq_table_delete(regions_table)
  regions_df <- 
    bq_project_query(bq_project, 
                     regions_sql,
                     destination_table = regions_table,
                     use_legacy_sql = FALSE,
                     allowLargeResults = TRUE) %>%
    bq_table_download()
    
  
}else {
  bq_table_create(regions_table)
  regions_df <- 
    bq_project_query(bq_project, 
                     query = regions_sql,
                     destination_table = regions_table,
                     use_legacy_sql = FALSE,
                     allowLargeResults = TRUE) %>%
    bq_table_download()
}

regions_df_out <- regions_df %>%
  dplyr::select(-created_at, -updated_at, -redirect_id, -best_pic_ids, -verified_user_id, -verified_date)

# Save locally
write_csv(regions_df_out, file.path(emlab_project_dir, "data", "01-raw", "diveboard", "regions.csv"))
```

## shops

The raw `shops.sql` file contains 11,400 rows and 37 columns: 
- id
- source
- source_id
- kind
- lat
- lng
- name
- address
- email
- web
- phone
- desc
- created_at
- updated_at
- moderate
- shop_vanity
- category
- about_html
- city
- country_code
- facebook
- twitter
- google_plus
- openings
- nearby
- private_user_id
- flag_moderate_private_to_public
- google_geocode
- realm_dive
- realm_home
- realm_gear
- realm_travel
- paypal_id
- paypal_token
- paypal_secret
- score
- delta

I'm not sure if this will be helpful to us, but it definitely will be to our collaborators. Let's keep in in the emLab GCP for now and we can clean this up later. 

```{r}
shops_sql <- "
  SELECT
    *
  FROM
     EXTERNAL_QUERY('projects/emlab-gcp/locations/us-west2/connections/diveboard',
    'SELECT * FROM diveboard.shops;')"

shops_table <- bq_table(project = bq_project, 
                        table = "shops",
                        dataset = "diveboard")

if(bq_table_exists(shops_table)){
  
  bq_table_delete(shops_table)
  shops_df <- 
    bq_project_query(bq_project, 
                     shops_sql,
                     destination_table = shops_table,
                     use_legacy_sql = FALSE,
                     allowLargeResults = TRUE) %>%
    bq_table_download()
    
  
}else {
  bq_table_create(shops_table)
  shops_df <- 
    bq_project_query(bq_project, 
                     query = shops_sql,
                     destination_table = shops_table,
                     use_legacy_sql = FALSE,
                     allowLargeResults = TRUE) %>%
    bq_table_download()
}

# Save locally
write_csv(shops_df, file.path(emlab_project_dir, "data", "01-raw", "diveboard", "shops.csv"))
```

## spots

The raw `spots.sql` file contains 11,400 rows and 37 columns: 
- id
- name
- created_at
- updated_at
- lat
- long
- zoom
- moderate_id
- precise
- description
- map
- location_id
- region_id
- country_id
- private_user_id
- flag_moderate_private_to_public
- verified_user_id
- verified_date
- redirect_id
- from_bulk
- within_country_bounds
- best_pic_ids
- score
- delta

This is really what we want. Clearly this links to our location, region, and country lookup tables. 

```{r}
spots_sql <- "
  SELECT
      *
    FROM
     EXTERNAL_QUERY('projects/emlab-gcp/locations/us-west2/connections/diveboard',
    'SELECT * FROM diveboard.spots;')"

spots_table <- bq_table(project = bq_project, 
                            table = "spots",
                            dataset = "diveboard")

if(bq_table_exists(spots_table)){
  
  bq_table_delete(spots_table)
  spots_df <- 
    bq_project_query(bq_project, 
                     spots_sql,
                     destination_table = spots_table,
                     use_legacy_sql = FALSE,
                     allowLargeResults = TRUE) %>%
    bq_table_download()
    
  
}else {
  bq_table_create(spots_table)
  spots_df <- 
    bq_project_query(bq_project, 
                     query = spots_sql,
                     destination_table = spots_table,
                     use_legacy_sql = FALSE,
                     allowLargeResults = TRUE) %>%
    bq_table_download()
}

# Save locally
write_csv(spots_df, file.path(emlab_project_dir, "data", "01-raw", "diveboard", "spots.csv"))
```

# Problem Files

We have two files (dives_filtered.sql and users_filtered.sql) that were unable to be imported via the method we used above because both files were the result of SQL queries instead of Heidi SQL dumps and we're missing the table schema at the top. Unfortunately we really need one of them, so we'll have to do this manually. Let's start by revisiting the earlier methods and see if we can piece together a way of extracting this data. 

## dives_filtered

```{r}
# Original file is stored on Google Shared Drive (DO NOT EDIT DIRECTLY)
diveboard_dat_path <- file.path(emlab_data_dir, "diveboard-dive-sites")

# Let's look at our first file by just reading it in as text
dives_filtered <- read_file(paste0(diveboard_dat_path, "dives_filtered.sql"))

# I can manually pick out the table schema, so let's see if we can extract those using some string matching... 
dives_filtered_intro <- str_extract(dives_filtered, "^.*(?=(VALUES))")

# Now let's extract everything between the parentheses
dives_filtered_schema <- str_extract(dives_filtered_intro, "\\([^()]+\\)")
dives_filtered_schema_clean <- substring(dives_filtered_schema, 2, nchar(dives_filtered_schema)-1)

# That worked, so we can go ahead and turn this into a vector of column names for our table
dives_filtered_col_names <- unlist(str_split(dives_filtered_schema_clean, ","))

# Now let's extract the actual data values using some string matching...
dives_filtered_values <- str_extract(dives_filtered, "(?<=VALUES)(?s)(.*$)")

# Now pull out everything in the parentheses into individual list items
dives_filtered_data <- str_match_all(dives_filtered_values, "\\([^()]+\\)")[[1]]
dives_filtered_data_df <- as_tibble(dives_filtered_data) %>%
  mutate(V2 = substring(V1, 2, nchar(V1)-1))

# # Now let's test some regex to figure out how to split this...
# test_df <- dives_filtered_data_df %>%
#   slice(80:100)
# 
# test_string <- test_df$V2[1]
# 
# test_replace <- stringr::str_replace_all(test_string, "\\'[^\\']*\\'", function(x) {
#   stringr::str_replace_all(x, ",", "+")})
# 
# # Try applying it to 20 entries
# test_df_out <- test_df %>%
#   mutate(V3 = stringr::str_replace_all(V2, "\\'[^\\']*\\'", function(x) {
#   stringr::str_replace_all(x, ",", "+")}),
#          V4 = str_split(V3, ",")) %>%
#   rowwise() %>%
#   mutate(length_list = length(V4))
# 
# # Yes I think we're good!

# Do some final wrangling of our entire dataframe (Takes ~ 20 min)
dives_filtered_data_out <- dives_filtered_data_df %>%
  mutate(V3 = stringr::str_replace_all(V2, "\\'[^\\']*\\'", function(x) {
  stringr::str_replace_all(x, ",", "+")}),
         V4 = str_split(V3, ",")) %>%
  rowwise() %>%
  mutate(length_list = length(V4))

# How many do we keep if we only keep the onles with 80 items? 310,105 out of 374,979. Good start
dives_filtered_keep <- dives_filtered_data_out %>%
  dplyr::filter(length_list == 80) %>%
    dplyr::select(-c(V1, V2, V3, length_list)) %>%
    rename(list_run = V4)

bp <- nrow(dives_filtered_keep)/10

for(i in 1:10){
  
  start <- floor(bp*(i-1))
  end <- ceiling(bp*i)
  
  dives_filtered_df <- dives_filtered_keep %>%
  dplyr::slice((start+1):end) %>%
  unnest_wider(list_run)
  
  colnames(dives_filtered_df) <- dives_filtered_col_names
  saveRDS(dives_filtered_df, file = file.path(emlab_project_dir, "data", "01-raw", "diveboard", "dives_filtered_", i, ".rds"))
}

# # Now keep only what we want and spread our list column into individual columns (Takes ~ 1 hour)
# dives_filtered_df_1 <- dives_filtered_keep %>%
#   dplyr::slice(1:bp) %>%
#   unnest_wider(list_run)
# 
# colnames(dives_filtered_df_1) <- dives_filtered_col_names
# saveRDS(dives_filtered_df_1, file = file.path(emlab_project_dir, "data", "01-raw", "diveboard", "dives_filtered_1.rds"))
# 
# dives_filtered_df_1 <- dives_filtered_keep %>%
#   dplyr::slice(1:bp) %>%
#   unnest_wider(list_run)
# 
# colnames(dives_filtered_df_1) <- dives_filtered_col_names
# saveRDS(dives_filtered_df_1, file = file.path(emlab_project_dir, "data", "01-raw", "diveboard", "dives_filtered_1.rds"))
# #write_csv(dives_filtered_df_1, file.path(emlab_project_dir, "data", "01-raw", "diveboard", "dives_filtered_1.csv"))
# 
# dives_filtered_df_2 <- dives_filtered_keep %>%
#   dplyr::slice(((break_point/2)+1):break_point) %>%
#   unnest_wider(list_run)
# 
# colnames(dives_filtered_df_2) <- dives_filtered_col_names
# saveRDS(dives_filtered_df_2, file = file.path(emlab_project_dir, "data", "01-raw", "diveboard", "dives_filtered_2.rds"))
```

