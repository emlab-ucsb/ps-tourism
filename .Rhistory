transformed_stockdistrib %>% filter(ocean==0) #ok, they are all ocean.
#plot(transformed_stockdistrib$f_highly_mpa) # use 0.5 as threshold for highly protected MPA.
ocean_coordinates <- transformed_stockdistrib %>% select(cell_id,lon,lat,f_highly_mpa)
ocean_coordinates %>% ggplot(aes(x=lon,y=lat,fill=1)) + geom_raster()
##load EEZ file
eez_mollweide <- readRDS(file = here("data","eez_mollweide.rds"))
head(eez_mollweide)
#cell id with country names
cell_id_with_country <- left_join(ocean_coordinates,eez_mollweide,by=c("lon","lat"))
#--- load MegaData --- add biomass density
MegaData<-readRDS(here("data","MegaData_Ray.rds"))
MegaData_filtered <- MegaData %>% filter(INCLUDE==1) %>% mutate(bvk_fin = 1-(ExploitationRate_BAU1_Ray/r_fin)) %>% dplyr::select(stockid,SciName,r_fin,Kfin,bvk_fin)
#ensure no negative numbers
MegaData_filtered$bvk_fin[MegaData_filtered$bvk_fin < 0] <- 0# Set very small negative values to 0
MegaData_filtered$ID <- seq.int(nrow(MegaData_filtered)) #add ID number
head(MegaData_filtered)
tail(MegaData_filtered)
dim(MegaData_filtered)
MegaData_filtered$check_stock_id<-colnames(transformed_stockdistrib)[6:1155] #ok. This is just a check that the files are matched.
#--- load the distance matrix (source, sink, distance)
merged_dist_matrix<-readRDS(here("data","distance-library","merged_dist_matrix","merged_dist_matrix.rds"))
#convert distance from m to km
merged_dist_matrix$distance <- merged_dist_matrix$distance/1000
head(merged_dist_matrix)
##This is an old code -- Not in use
# #add MPA info: i.e., MPA_sink and MPA_source... whether sink or sources are MPAs.
# MPA_source <- transformed_stockdistrib %>% dplyr::select(cell_id,f_highly_mpa) %>% mutate(f_highly_mpa = 1*(f_highly_mpa>=0.5)) %>% dplyr::rename(source=cell_id, MPA_source=f_highly_mpa)
# MPA_sink <- MPA_source %>% dplyr::rename(sink=source, MPA_sink=MPA_source)
# merged_dist_matrix_source <- left_join(merged_dist_matrix,MPA_source,by="source")
# merged_dist_matrix_source_sink <- left_join(merged_dist_matrix_source,MPA_sink,by="sink")
# dim(merged_dist_matrix_source_sink)
# #merging data option 1
# MPA<-c(1:100000)
# ptm <- proc.time()
# merged_dist_matrix_source_sink %>% mutate(MPA_sink=replace(MPA_sink,sink %in% MPA,1)) %>% head()
# (proc.time() - ptm)/60 #check process time in minutes
#
# #merging data option 2
# dataA <- merged_dist_matrix %>% as.data.table()
# dataB <- MPA_sink %>% as.data.table()
# ptm <- proc.time()
# dataA[dataB,on="sink"]
# (proc.time() - ptm)/60 #check process time in minutes
#--load homerange and pld data predictions then process
pld_data <- read.csv(here("data","homerange_pld_predictions","pld_rf_predictions_final.csv")) %>% dplyr::select(species,observed_PLD,predicted_PLD) %>% dplyr::rename(SciName = species)
#summarize in case of duplicates
pld_data_mean <- pld_data %>% group_by(SciName) %>% summarise(mean_observed_PLD = mean(observed_PLD), sd_observed_PLD = sd(observed_PLD), mean_predicted_PLD = mean(predicted_PLD))
head(pld_data_mean)
dim(pld_data_mean)
homerange_data <- read.csv(here("data","homerange_pld_predictions","homerange_rf_predictions_10112022.csv")) %>% dplyr::select(species,observed_homerange,predicted_homerange) %>% dplyr::rename(SciName = species)
homerange_data_mean <- homerange_data %>% group_by(SciName) %>% summarise(mean_observed_homerange = mean(observed_homerange), sd_observed_homerange = sd(observed_homerange), mean_predicted_homerange = mean(predicted_homerange))
head(homerange_data_mean)
species_list <- MegaData_filtered %>% dplyr::select(SciName) %>% unique()
dim(species_list)
db_with_pld <- left_join(species_list,pld_data_mean,by="SciName")
db_with_pld_hrange <- left_join(db_with_pld,homerange_data_mean)
head(db_with_pld_hrange)
dim(db_with_pld_hrange)
db_with_pld_hrange_filtered <- db_with_pld_hrange %>% mutate(PLD = ifelse(!is.na(mean_observed_PLD),mean_observed_PLD,mean_predicted_PLD), homerange = ifelse(!is.na(mean_observed_homerange),mean_observed_homerange,mean_predicted_homerange),
complete = ifelse((PLD>0 & homerange>0),1,0))
sum(db_with_pld_hrange_filtered$complete,na.rm=T) #610 species out of 811. Check what proportion of K this is
#merge with the full data list
MegaData_PLD_hrange <- left_join(MegaData_filtered,db_with_pld_hrange_filtered,by="SciName")
#% of the original biomass considered.
MegaData_PLD_hrange %>% filter(complete==1) %>% summarize(sum(Kfin))/sum(MegaData_PLD_hrange$Kfin) #we have data for 81% of the K.
#We can remove entries with incomplete data as we rely on relative biomass metric rather than absolute.
###--- Add family info
# #load the PLD data. Load home range.
# pld_data <- read.csv(here("data","mobility_pld_imputed.csv")) %>% dplyr::select(SciName,pld)
#
# #--!!! This is the checking of the home range data.
# homerange_data <- read.csv(here("data","homerange","homerange_rf_predictions.csv")) %>% distinct(species, .keep_all = TRUE) %>% dplyr::select(species,pred) %>% dplyr::rename(SciName = species, homerange = pred)
# # max(homerange_data$pred)
# # dim(homerange_data)
# # head(homerange_data)
# #check homerange data# ok, there are 651 predicted values only.
# test<-left_join(pld_data,homerange_data,by="SciName")
# sum(!is.na(test$homerange)*1)
# #--!!!
#max dispersal distance limit (3 sigma larvae)
3*1.33*(max(MegaData_PLD_hrange$PLD,na.rm=T)^1.3)*sqrt(pi/2) #18K
#merge
MegaData_filtered_step2 <- MegaData_PLD_hrange %>% mutate(sigma_larvae = 1.33*(PLD^1.3)*sqrt(pi/2), dispersal_distance_limit = 3*sigma_larvae, homerange_radius = sqrt(homerange/pi))
head(MegaData_filtered_step2)
#add the geographic range of the stock
full_stock_distrib <- transformed_stockdistrib[6:1155]
geog_range_perstock<- colSums(full_stock_distrib,na.rm=T) %>% as.data.frame()
colnames(geog_range_perstock) <- c('geog_range')
geog_range_perstock$stockid <- row.names(geog_range_perstock)
head(geog_range_perstock)
MegaData_filtered_step3 <- left_join(MegaData_filtered_step2,geog_range_perstock,by="stockid")
MegaData_filtered_step_fin <- MegaData_filtered_step3 %>% mutate(Kperpixel = Kfin/geog_range)
head(MegaData_filtered_step_fin)
sum(MegaData_filtered_step_fin$complete, na.rm = T)
min(MegaData_filtered_step_fin$dispersal_distance_limit,na.rm=T)
##--Determine the stocks that will be included in our analysis. The stocks will be those that intersects with any of the dive sites and those with complete parameters.
#We can remove stocks with no intersection with diving
# Load the dive suitability layer
dive_suitability <- read.csv(here("data","dive","dive_suitability_by_cell.csv"))
head(dive_suitability)
dim(dive_suitability)#1997 dive sites!!!
#Fraction and absolute size of global ocean suitable for diving
dim(dive_suitability)[1]*100/dim(transformed_stockdistrib)[1] #this is the fraction of ocean with diving. 1.34%
dim(dive_suitability)[1]*50*50# 5 million km2. This is the total area of ocean surface with diving.
#Identify the stock that intersects with diving
head(transformed_stockdistrib)
filter1<-transformed_stockdistrib %>% filter(cell_id %in% dive_suitability$cell_id)
dim(filter1)#there are 2197 dive sites
check<-filter1[,6:1155]
dim(check)
mysum<-colSums(check,na.rm=T) %>% as.data.frame()
head(mysum)
names(mysum)[1] <- "n_pixel_intersect"
mysum %>% filter(n_pixel_intersect==0) %>% dim() ##remove 94 stocks with no intersection with diving!
stocklist_with_diving <- which(mysum!=0)
length(stocklist_with_diving)
#now, how about stock list with complete parameters?
stocklist_complete_params <- which(MegaData_filtered_step_fin$complete==1)
length(stocklist_complete_params)
#our final stocklist will be the intersection of the two
stocklist <- stocklist_complete_params[stocklist_complete_params %in% stocklist_with_diving]
length(stocklist)#so there are 821 stocks with complete information
#how many species?
length(unique(MegaData_filtered_step_fin[stocklist,]$SciName))#600 species
Checkme <- MegaData_filtered_step_fin[stocklist,]
ggplot(Checkme, aes(x=mean_observed_PLD, y= mean_predicted_PLD)) + geom_point() + geom_abline(slope=1, intercept=0)
#Pristine seas tourism-MPA model
#Last checked: 2 Nov 2022
#Ren Cabral
#This code evaluates the build-up of biomass inside MPAs, evaluate biodiversity change, and compute the corresponding dive tourism benefits
gc()
rm(list = ls())
library(doParallel)
library(raster)
library(rgdal)
library(maptools)
library(dplyr)
library(cowplot)
library(reshape)
library(scales)
library(maps)
library(sf)
library(tidyverse)
library(patchwork)
library(data.table)
library(here)
library(fst)
library(ggspatial)
library(ggrepel)
library(bigstatsr)
#-- Path to the Pristine Seas tourism directory on the emLab Google Drive
this_project_dir <- "/Volumes/GoogleDrive/Shared drives/emlab/projects/current-projects/ps-tourism"
#-- load stock distribution
transformed_stockdistrib <- readRDS(here("data","transformed_stockdistrib.rds"))
head(transformed_stockdistrib)
dim(transformed_stockdistrib) #149547 by 1155
#check if all cell_id's are ocean id
transformed_stockdistrib %>% filter(ocean==0) #ok, they are all ocean.
#plot(transformed_stockdistrib$f_highly_mpa) # use 0.5 as threshold for highly protected MPA.
ocean_coordinates <- transformed_stockdistrib %>% select(cell_id,lon,lat,f_highly_mpa)
ocean_coordinates %>% ggplot(aes(x=lon,y=lat,fill=1)) + geom_raster()
##load EEZ file
eez_mollweide <- readRDS(file = here("data","eez_mollweide.rds"))
head(eez_mollweide)
#cell id with country names
cell_id_with_country <- left_join(ocean_coordinates,eez_mollweide,by=c("lon","lat"))
#--- load MegaData --- add biomass density
MegaData<-readRDS(here("data","MegaData_Ray.rds"))
MegaData_filtered <- MegaData %>% filter(INCLUDE==1) %>% mutate(bvk_fin = 1-(ExploitationRate_BAU1_Ray/r_fin)) %>% dplyr::select(stockid,SciName,r_fin,Kfin,bvk_fin)
#ensure no negative numbers
MegaData_filtered$bvk_fin[MegaData_filtered$bvk_fin < 0] <- 0# Set very small negative values to 0
MegaData_filtered$ID <- seq.int(nrow(MegaData_filtered)) #add ID number
head(MegaData_filtered)
tail(MegaData_filtered)
dim(MegaData_filtered)
MegaData_filtered$check_stock_id<-colnames(transformed_stockdistrib)[6:1155] #ok. This is just a check that the files are matched.
#--- load the distance matrix (source, sink, distance)
merged_dist_matrix<-readRDS(here("data","distance-library","merged_dist_matrix","merged_dist_matrix.rds"))
#convert distance from m to km
merged_dist_matrix$distance <- merged_dist_matrix$distance/1000
head(merged_dist_matrix)
#--load homerange and pld data predictions then process
pld_data <- read.csv(here("data","homerange_pld_predictions","pld_rf_predictions_final.csv")) %>% dplyr::select(species,observed_PLD,predicted_PLD) %>% dplyr::rename(SciName = species)
#summarize in case of duplicates
pld_data_mean <- pld_data %>% group_by(SciName) %>% summarise(mean_observed_PLD = mean(observed_PLD), sd_observed_PLD = sd(observed_PLD), mean_predicted_PLD = mean(predicted_PLD))
head(pld_data_mean)
dim(pld_data_mean)
homerange_data <- read.csv(here("data","homerange_pld_predictions","homerange_rf_predictions_10112022.csv")) %>% dplyr::select(species,observed_homerange,predicted_homerange) %>% dplyr::rename(SciName = species)
homerange_data_mean <- homerange_data %>% group_by(SciName) %>% summarise(mean_observed_homerange = mean(observed_homerange), sd_observed_homerange = sd(observed_homerange), mean_predicted_homerange = mean(predicted_homerange))
head(homerange_data_mean)
species_list <- MegaData_filtered %>% dplyr::select(SciName) %>% unique()
dim(species_list)
db_with_pld <- left_join(species_list,pld_data_mean,by="SciName")
db_with_pld_hrange <- left_join(db_with_pld,homerange_data_mean)
head(db_with_pld_hrange)
dim(db_with_pld_hrange)
db_with_pld_hrange_filtered <- db_with_pld_hrange %>% mutate(PLD = ifelse(!is.na(mean_observed_PLD),mean_observed_PLD,mean_predicted_PLD), homerange = ifelse(!is.na(mean_observed_homerange),mean_observed_homerange,mean_predicted_homerange),
complete = ifelse((PLD>0 & homerange>0),1,0))
sum(db_with_pld_hrange_filtered$complete,na.rm=T) #610 species out of 811. Check what proportion of K this is
#merge with the full data list
MegaData_PLD_hrange <- left_join(MegaData_filtered,db_with_pld_hrange_filtered,by="SciName")
#% of the original biomass considered.
MegaData_PLD_hrange %>% filter(complete==1) %>% summarize(sum(Kfin))/sum(MegaData_PLD_hrange$Kfin) #we have data for 81% of the K.
#max dispersal distance limit (3 sigma larvae)
3*1.33*(max(MegaData_PLD_hrange$PLD,na.rm=T)^1.3)*sqrt(pi/2) #18K
#merge
MegaData_filtered_step2 <- MegaData_PLD_hrange %>% mutate(sigma_larvae = 1.33*(PLD^1.3)*sqrt(pi/2), dispersal_distance_limit = 3*sigma_larvae, homerange_radius = sqrt(homerange/pi))
head(MegaData_filtered_step2)
#add the geographic range of the stock
full_stock_distrib <- transformed_stockdistrib[6:1155]
geog_range_perstock<- colSums(full_stock_distrib,na.rm=T) %>% as.data.frame()
colnames(geog_range_perstock) <- c('geog_range')
geog_range_perstock$stockid <- row.names(geog_range_perstock)
head(geog_range_perstock)
MegaData_filtered_step3 <- left_join(MegaData_filtered_step2,geog_range_perstock,by="stockid")
MegaData_filtered_step_fin <- MegaData_filtered_step3 %>% mutate(Kperpixel = Kfin/geog_range)
head(MegaData_filtered_step_fin)
sum(MegaData_filtered_step_fin$complete, na.rm = T)
min(MegaData_filtered_step_fin$dispersal_distance_limit,na.rm=T)
##--Determine the stocks that will be included in our analysis. The stocks will be those that intersects with any of the dive sites and those with complete parameters.
#We can remove stocks with no intersection with diving
# Load the dive suitability layer
dive_suitability <- read.csv(here("data","dive","dive_suitability_by_cell.csv"))
head(dive_suitability)
dim(dive_suitability)#1997 dive sites!!!
#Fraction and absolute size of global ocean suitable for diving
dim(dive_suitability)[1]*100/dim(transformed_stockdistrib)[1] #this is the fraction of ocean with diving. 1.34%
dim(dive_suitability)[1]*50*50# 5 million km2. This is the total area of ocean surface with diving.
#Identify the stock that intersects with diving
head(transformed_stockdistrib)
filter1<-transformed_stockdistrib %>% filter(cell_id %in% dive_suitability$cell_id)
dim(filter1)#there are 2197 dive sites
check<-filter1[,6:1155]
dim(check)
mysum<-colSums(check,na.rm=T) %>% as.data.frame()
head(mysum)
names(mysum)[1] <- "n_pixel_intersect"
mysum %>% filter(n_pixel_intersect==0) %>% dim() ##remove 94 stocks with no intersection with diving!
stocklist_with_diving <- which(mysum!=0)
length(stocklist_with_diving)
#now, how about stock list with complete parameters?
stocklist_complete_params <- which(MegaData_filtered_step_fin$complete==1)
length(stocklist_complete_params)
#our final stocklist will be the intersection of the two
stocklist <- stocklist_complete_params[stocklist_complete_params %in% stocklist_with_diving]
length(stocklist)#so there are 821 stocks with complete information
#how many species?
length(unique(MegaData_filtered_step_fin[stocklist,]$SciName))#600 species
##Check final list of dataset
Checkme <- MegaData_filtered_step_fin[stocklist,]
ggplot(Checkme, aes(x=mean_observed_PLD, y= mean_predicted_PLD)) + geom_point() + geom_abline(slope=1, intercept=0)
#connectivity matrix, larvae
run_subset_connectivitymatrix <- 0 #1 for turn on, 0 to switch off
# #NOTE-- JUST OPEN THIS WHEN THE DISTANCE MATRICES CHANGES
# #check if we can load individual files here then merge
# #distmat_filenames <- list.files(path=here("data","distance-library","fst_file"), pattern=".fst", all.files=FALSE,full.names=TRUE)
# distmat_filenames <- list.files(path=here("data","distance-library"), pattern=".rds", all.files=FALSE,full.names=TRUE)
# distmat_filenames[1]
# length(distmat_filenames)#40 files
#
# #let us load all the distance matrices, subset, then save as fst file.
# for (i in 1:length(distmat_filenames)){
#   myfile <- readRDS(distmat_filenames[i]) %>% filter(distance<=3667657.8)
#   write.fst(myfile , here("data","distance-library","dist_matrix_subset_fst",paste0(i,"_connect_matrix.fst")))
# }
distmat_filenames_subset_fst <- list.files(path=here("data","distance-library","dist_matrix_subset_fst"), pattern=".fst", all.files=FALSE,full.names=TRUE)
distmat_filenames_subset_fst[1]
length(distmat_filenames_subset_fst)#40 files
source(here("scripts", "functions","func_stitch_connect_matrix.R"))
#parallel version
if(run_subset_connectivitymatrix == 1){
stocklist2 <- c(461,467,473,475,478,stocklist[393:821])
nstock<-length(stocklist2)
registerDoParallel(6)
#stocklist2 <- 10#stocklist[10:nstock]
#registerDoParallel(detectCores()/2)
foreach(stock_num=stocklist2) %dopar% {
stock_subset_i <- which(transformed_stockdistrib[,stock_num+5] > 0)
##--! this is the code that can be used when the maximum disperal is over 1K km.
distance_mat_full_prop_larvae <- func_stitch_connect_matrix(distmat_files=distmat_filenames_subset_fst,stock_subset_i=stock_subset_i,MegaData_filtered_step2=MegaData_filtered_step2,stock_num=stock_num)
##-- merged_dist_matrix is in km
#distance_mat_full_prop_larvae <- merged_dist_matrix %>% filter(source %in% stock_subset_i, sink %in% stock_subset_i, distance<=MegaData_filtered_step2$dispersal_distance_limit[stock_num]) %>% group_by(source) %>% mutate(biom_prop = exp(-( distance^2 / (2*(MegaData_filtered_step2$sigma_larvae[stock_num]^2))) ), biom_prop = biom_prop/sum(biom_prop)) %>% dplyr::select(-distance) %>% as.data.table()
#fts is the fastest in saving and loading files.
fst::write.fst(distance_mat_full_prop_larvae , paste0("/Users/ren/Documents/GitHub/tourism-mpa-support/connect_matrix/",stock_num,"_connect_larvae.fst"))
}
doParallel::stopImplicitCluster()
}
#check how many files
connect_matrix_nfiles <- list.files(path="/Users/ren/Documents/GitHub/tourism-mpa-support/connect_matrix/", pattern=".fst", all.files=FALSE,full.names=TRUE)
length(connect_matrix_nfiles)#821 files
for (i in 1:length(connect_matrix_nfiles)){
check<-read.fst(connect_matrix_nfiles[i])
print(dim(check)[1])
}
##check the output
#seeme <- read.fst("/Users/ren/Documents/GitHub/tourism-mpa-support/connect_matrix/1_connect_larvae.fst")
#seeme %>% group_by(source) %>% summarize(sum(biom_prop))
#seeme %>% group_by(sink) %>% summarize(sum(biom_prop))
#connectivity matrix, adult
run_subset_connectivitymatrix_adult <- 0 #1 for on, 0 to switch this off
if(run_subset_connectivitymatrix_adult == 1){
registerDoParallel(detectCores()/2)
foreach(stock_num=stocklist) %dopar% {
stock_subset_i <- which(transformed_stockdistrib[,stock_num+5] > 0)
#included here: filter distance
distance_mat_full_prop_adult <- merged_dist_matrix %>% filter(source %in% stock_subset_i, sink %in% stock_subset_i, distance<=MegaData_filtered_step2$homerange_radius[stock_num]) %>% group_by(source) %>% mutate(biom_prop = 1/n()) %>% dplyr::select(-distance) %>% as.data.table()
# distance_mat_full_prop <- distance_mat_full %>% filter(pos1 %in% stock_subset_i, pos2 %in% stock_subset_i) %>% group_by(pos1) %>% mutate(biom_prop = exp(-( dist^2 / (2*(sigma^2))) ), biom_prop = biom_prop/sum(biom_prop)) %>% dplyr::select(-dist) %>% as.data.table()
#fts is the fastest in saving and loading files.
fst::write.fst(distance_mat_full_prop_adult , paste0("/Users/ren/Documents/GitHub/tourism-mpa-support/connect_matrix_adult/",stock_num,"_connect_adult.fst"))
}
doParallel::stopImplicitCluster()
}
##--for checking the code
#stock_subset_i <- which(transformed_stockdistrib[,255+5] > 0)
#distance_mat_full_prop_adult <- merged_dist_matrix %>% filter(source %in% stock_subset_i, sink %in% stock_subset_i, distance<=MegaData_filtered_step2$homerange_radius[stock_num]) %>% group_by(source) %>% mutate(biom_prop = 1/n()) %>% dplyr::select(-distance) %>% as.data.table()
#distance_mat_full_prop_adult %>% group_by(source) %>% summarize(sum(biom_prop))
#distance_mat_full_prop_adult %>% group_by(sink) %>% summarize(sum(biom_prop))
#test<-fst::read.fst(paste0("/Users/ren/Documents/GitHub/tourism-mpa-support/connect_matrix/1_connect_larvae.fst"))
#head(test)
#---- this is the main code that evaluates the biomass change for each stock and the change in biodiversity
#number of dives
number_of_dives <- read.csv(here("data","dive","number_of_dives_extrapolated_by_cell.csv"))
head(number_of_dives)
dim(number_of_dives)#1997
sum(number_of_dives$n_dives_extrap)#50700017 dives.
#revenue in billion
sum(number_of_dives$n_dives_extrap)*60/1000000000
hist(number_of_dives$n_dives_extrap)
ggplot(data.frame(log(number_of_dives$n_dives_extrap)), aes(log(number_of_dives$n_dives_extrap))) +     # Histogram with logarithmic axis
geom_histogram(bins=10)
dive_per_country <- left_join(number_of_dives,cell_id_with_country,by="cell_id")
iso_library<-dive_per_country %>% select(sovereign1,territory1,iso_sov1,iso_ter1) %>% unique() %>% arrange(sovereign1) %>% filter(!is.na(sovereign1))
#load country classification (SIDS, developing, etc.)
country_classification <- read.csv(here("data","UN_territory_sovereign_classification.csv"))
#country_classification$SIDS <-as.factor(country_classification$Classification)
country_classification_with_iso <- left_join(country_classification,iso_library,by=c("sovereign1","territory1"))
country_classification_kat <- read.csv(here("data","country_status_lookup_manual_category.csv")) %>% mutate(Classification_kat = ifelse(manual_development_status=="Developed", "Developed", "Developing")) %>%
select(iso3,Classification_kat) %>% dplyr::rename(iso_ter1 = iso3)
#note from Kat: use "development_status" -- developed and others.
country_classification_with_iso_and_class <- left_join(country_classification_with_iso,country_classification_kat,by="iso_ter1") %>% mutate(match = (Classification==Classification_kat))
#plot number of dives per country
head(dive_per_country)
plot_number_dives <- dive_per_country %>% group_by(territory1) %>% summarize(n_dive=sum(n_dives_extrap)) %>% left_join(country_classification,by="territory1") %>%
arrange(-n_dive) %>% slice(1:50) %>%
#ggplot(aes(x=as.factor(territory1),y=n_dive)) + geom_col()
ggplot(aes(x = reorder(as.factor(territory1), n_dive/1000000), y = n_dive/1000000, fill=Classification))+
geom_bar(stat = "identity")+ theme_classic()+#theme_minimal()+#theme_classic()+
coord_flip()+ labs(y = "Dive per year, million")+theme(axis.title.y = element_blank())
plot_number_dives
#plot number of dive pixels per country
plot_number_divepexels_country <- dive_per_country %>% group_by(territory1) %>% summarize(n_divesites=n()) %>% filter(territory1!="NA") %>% left_join(country_classification,by="territory1") %>%
arrange(-n_divesites) %>% slice(1:50) %>%
#ggplot(aes(x=as.factor(territory1),y=n_dive)) + geom_col()
ggplot(aes(x = reorder(as.factor(territory1), n_divesites), y = n_divesites, fill=Classification))+
geom_bar(stat = "identity")+ theme_classic()+#theme_minimal()+#theme_classic()+
coord_flip()+ labs(y = "Number of dive site pixel")+theme(axis.title.y = element_blank())
plot_number_divepexels_country
#-- dive density might be interesting! # of dives per pixel per territory
ndivepixel_territory <- cell_id_with_country  %>% group_by(territory1) %>% summarize(total_territory_pixel=n())
#plot % of the eez that are number of dive sites
plot_divedensity <- dive_per_country %>% group_by(territory1) %>% summarize(n_dive=sum(n_dives_extrap)) %>% left_join(ndivepixel_territory,by="territory1") %>% mutate(percent_divearea = n_dive/total_territory_pixel) %>% filter(territory1!="NA") %>%
arrange(-percent_divearea) %>% slice(1:50) %>%
#ggplot(aes(x=as.factor(territory1),y=n_dive)) + geom_col()
ggplot(aes(x = reorder(as.factor(territory1), percent_divearea), y = percent_divearea))+
geom_bar(stat = "identity")+ theme_classic()+#theme_minimal()+#theme_classic()+
coord_flip()+ labs(y = "# yearly dives/pixel")+theme(axis.title.y = element_blank())
plot_divedensity
ggsave(here("figures","supplementary","plot_divedensity.jpg"),plot_divedensity, width = 10, height = 14, units = "cm")
# ##--correlate # of dives with on-reef values
# #number of dive per country
# ndive_per_country <- dive_per_country %>% group_by(territory1,iso_ter1,iso_sov1) %>% summarize(n_dive=sum(n_dives_extrap))
#
# #ndive_per_country$iso_ter1[ndive_per_country$territory1=="Alaska"]="USA"
# #ndive_per_country$iso_ter1[ndive_per_country$territory1=="Andaman and Nicobar"]="IND"
# #ndive_per_country$iso_ter1[ndive_per_country$territory1=="Azores"]="PRT"
#
# head(ndive_per_country)
# dim(ndive_per_country)
# #on-reef tourism value per territory from Spalding et al.
# onreef_values <- read.csv(here("data","tourism_reef_values","Tourvalues_Spalding.csv"))
# head(onreef_values)
# dim(onreef_values)
# #now perform inner join
# correlate_dive_and_value <- merge(x=ndive_per_country,y=onreef_values,by="territory1")
# dim(correlate_dive_and_value)
# head(correlate_dive_and_value)
#
# plot_correlate_dive_and_value<- ggplot(correlate_dive_and_value, aes(x=OnReef/1000000,y=n_dive/1000000))+geom_point()+geom_smooth(method = lm,colour="gray")+
#   geom_text_repel(aes(OnReef/1000000, n_dive/1000000, label = territory1), size = 3)+
#   labs(x="On-reef tourism value, billion US$", y = "Dive per year, million")+theme_classic()
# plot_correlate_dive_and_value
ndive_per_sovereign <- dive_per_country %>% group_by(sovereign1,iso_sov1) %>% summarize(n_dive=sum(n_dives_extrap))
head(ndive_per_sovereign)
onreef_values <- read.csv(here("data","tourism_reef_values","Tourvalues_Spalding.csv")) %>% group_by(iso_sov1) %>% summarise(onreef_value=sum(OnReef))
head(onreef_values)
correlate_dive_and_value <- merge(x=ndive_per_sovereign,y=onreef_values,by="iso_sov1")
head(correlate_dive_and_value)
plot_correlate_dive_and_value<- ggplot(correlate_dive_and_value, aes(x=onreef_value/1000000,y=n_dive/1000000))+geom_point()+geom_smooth(method = lm,colour="gray")+
geom_text_repel(aes(onreef_value/1000000, n_dive/1000000, label = sovereign1), size = 3)+
labs(x="On-reef tourism value, billion US$", y = "Dive per year, million")+theme_classic()
plot_correlate_dive_and_value
##--correlate # of dives with flikr data
ndive_per_sovereign <- dive_per_country %>% group_by(sovereign1,iso_sov1) %>% summarize(n_dive=sum(n_dives_extrap))
head(ndive_per_sovereign)
#flickr_data <- read.csv(here("data","flickr","flickr_aggregate_by_country.csv"))
flickr_data <- read.csv(here("data","flickr","flickr_webscraped_data_raw_v11.csv"))
flickr_data_sum <- flickr_data %>% group_by(iso_code) %>% summarize(count=n()) %>% dplyr::rename(iso_sov1=iso_code)
head(flickr_data_sum)
correlate_dive_and_flickr <- merge(x=ndive_per_sovereign,y=flickr_data_sum,by="iso_sov1") %>% filter(is.na(iso_sov1)==F)
head(correlate_dive_and_flickr)
plot_correlate_dive_and_flickr <- ggplot(correlate_dive_and_flickr, aes(x=count,y=n_dive/1000000))+geom_point()+geom_smooth(method = lm,colour="gray")+
geom_text_repel(aes(count, n_dive/1000000, label = sovereign1), size = 3)+
labs(x="Number of flickr photos", y = "Dive per year, million")+theme_classic()
#figure 1 main
figure1<-cowplot::plot_grid(plot_number_divepexels_country, plot_number_dives,plot_correlate_dive_and_value,plot_correlate_dive_and_flickr, ncol = 2, labels = "AUTO",rel_heights=c(1,0.5))
figure1
ggsave(here("figures","main","plot_number_dives.jpg"),figure1, width = 20, height = 20, units = "cm")
#save a country file for me to assign country development categorization
head(dive_per_country)
##---[no need to run] dive_per_country %>% select(sovereign1) %>% unique() %>% write.csv(.,file = here("data","country_classification.csv"))
##---[no need to run] dive_per_country %>% select(territory1) %>% unique() %>% write.csv(.,file = here("data","territory_classification.csv"))
##---[no need to run] dive_per_country %>% select(sovereign1,territory1) %>% unique() %>% arrange(sovereign1) %>% filter(!is.na(sovereign1)) %>% write.csv(.,file = here("data","territory_sovereign_classification.csv"))
#checkme <- read.csv(here("data","UN_territory_sovereign_classification.csv"))
##--price per dive
prices_constant_by_cell <- read.csv(here("data","dive","prices_constant_by_cell.csv"))
dim(prices_constant_by_cell)
plot(prices_constant_by_cell$price) #60 USD per dive
#price per country
prices_country_region_by_cell <- read.csv(here("data","dive","prices_country_region_by_cell.csv"))
plot(prices_country_region_by_cell$price)
#request to add country name or country code in the file. Or a file of cell_id and country names.
#ok, use constant price.
ocean_coordinates_dive_suitable <- left_join(ocean_coordinates,dive_suitability,by="cell_id")
ocean_coordinates_dive_suitable_v2 <- left_join(ocean_coordinates_dive_suitable,number_of_dives,by="cell_id")
#plot suitability later
ocean_coordinates_dive_suitable_v2 %>% mutate(suitable = replace_na(suitable,0)) %>% ggplot() + geom_raster(aes(x=lon,y=lat,fill=suitable)) + scale_fill_gradientn(colours=c("black","orange")) #ok, great
# MPA location -- assume that a pixel is an MPA is f_highly_mpa>=0.5. Consult this assumption
MPA_vec <- transformed_stockdistrib %>% dplyr::select(cell_id,f_highly_mpa) %>% mutate(f_highly_mpa = (f_highly_mpa>=0.5))
MPA_loc <- MPA_vec %>% filter(f_highly_mpa=="TRUE") %>% select(cell_id)
#Question: How many of the suitable dive sites are already in MPAs?
divesite_in_MPA <- dive_suitability %>% filter(cell_id %in% MPA_loc$cell_id) %>% dim()
divesite_in_MPA#23 pixels.
divesite_in_MPA[1]*100/dim(number_of_dives)[1] #1.15% of the dive sites are inside MPA... hmmm... this sounds correct? maybe we should analyze it using high res data.
##--biodiversity prep
# Source functions
#sapply(list.files(pattern = "[.]R$", path = here::here("scripts", "functions"), full.names = TRUE),source)
#Load the functions
source(here("scripts", "functions","calculate_relative_bio_benefit.R"))
source(here("scripts", "functions","func_evaluateMPA_explicit.R"))
# Load data files necessary for biodiversity model
load(file = file.path(this_project_dir,  "data", "02-processed-data", "bio_model_input.RData"))
# set Z for biodiversity
z_bio <- 0.25
##--biomass prep file
nstock<-dim(MegaData_filtered)[1]
##---BIODIVERSITY CODE
# Calculate biodiversity benefit from today's protected cells
bio_benefit_current<-calculate_relative_bio_benefit(is_mpa_vect = MPA_vec$f_highly_mpa, v_out_matrix =  v_out_matrix,
v_in_matrix = v_in_matrix, weights  = bio_weights,
z_bio = z_bio, bau_benefit = bau_benefit, total_benefit_diff = total_benefit_diff)
#Biodiv benefits of zero MPA
MPA_vec$f_zero_mpa<-FALSE
bio_benefit_zero<-calculate_relative_bio_benefit(is_mpa_vect = MPA_vec$f_zero_mpa, v_out_matrix =  v_out_matrix,
v_in_matrix = v_in_matrix, weights  = bio_weights,
z_bio = z_bio, bau_benefit = bau_benefit, total_benefit_diff = total_benefit_diff)
#Biodiv benefits of 100% ocean in MPA
MPA_vec$f_all_mpa<-TRUE
bio_benefit_all<-calculate_relative_bio_benefit(is_mpa_vect = MPA_vec$f_all_mpa, v_out_matrix =  v_out_matrix,
v_in_matrix = v_in_matrix, weights  = bio_weights,
z_bio = z_bio, bau_benefit = bau_benefit, total_benefit_diff = total_benefit_diff)
#% increase from zero MPA to current MPA
(bio_benefit_current-bio_benefit_zero)*100/bio_benefit_zero
#% increase from current MPA to all MPA
(bio_benefit_all-bio_benefit_current)*100/bio_benefit_current
#biodiversity score, no MPA
bio_benefit_zero/max_benefit_allthreats #0.5326662
#biodiversity score, current MPA
bio_benefit_current/max_benefit_allthreats #0.5445721
#biodiversity score, all MPA
bio_benefit_all/max_benefit_allthreats #0.7563106
#% increase from current MPA to all MPA
(bio_benefit_all-bio_benefit_current)*100/bio_benefit_current
#% increase from current MPA to all threats solved (not needed)
(max_benefit_allthreats-bio_benefit_current)*100/bio_benefit_current
#Biodiv benefits of current MPAs + dive MPAs #do this next...
##----- BIOMASS CODE
#--test:
#func_evaluateMPA_explicit(stock_num=1, transformed_stockdistrib,MegaData_filtered_step_fin,MPA_loc)$biomass
stock_include <- stocklist#c(1,2,4,5,6)#stocklist[8] #c(1,2,4,5,6) #comment this. this is just a placeholder for building our code
ptm <- proc.time()
registerDoParallel(detectCores()/2)
#stock_include<-stocklist ##stocklist#1:nstock#c(1,3,5)#which(MegaData$INCLUDE==1)
collate_biomass_equi_merged <- foreach(stock_num=stock_include, .combine='cbind') %dopar% {
func_evaluateMPA_explicit(stock_num, transformed_stockdistrib,MegaData_filtered_step_fin,MPA_loc)$biomass
}
doParallel::stopImplicitCluster()
(proc.time() - ptm)/60 #check process time in minutes
colnames(collate_biomass_equi_merged)<-MegaData_filtered_step_fin$stockid[stock_include]
#head(collate_biomass_equi_merged)
dim(collate_biomass_equi_merged)
##--Calculate B/K per pixel
#compute K per pixel of our stock list
max(transformed_stockdistrib[6:1155],na.rm=T)
min(transformed_stockdistrib[6:1155],na.rm=T)
full_stock_distrib <- transformed_stockdistrib[6:1155]
filtered_stock_distrib <- full_stock_distrib[,stocklist] #this is the stock distrib of our filtered stock. Max value is 1 and with NAs
dim(filtered_stock_distrib)
#K/geogrange
Kmultiplyer <- MegaData_filtered_step_fin %>% select(Kperpixel) %>% slice(stocklist) %>% data.frame()
head(Kmultiplyer)
dim(Kmultiplyer)
df_Kmultiplyer <- t(data.frame(rep(Kmultiplyer,each=149547)))
